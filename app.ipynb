{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-hanging",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:52:59.974Z"
    }
   },
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "from scipy import ndimage\n",
    "from skimage.measure import regionprops\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu   # For finding the threshold for grayscale to binary conversion\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-application",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:52:59.981Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#get paths of each file in folder named Images\n",
    "#Images here contains my data(folders of various persons)\n",
    "imagePaths = list(paths.list_images('Images'))\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    # load the input image and convert it from BGR (OpenCV ordering)\n",
    "    # to dlib ordering (RGB)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #Use Face_recognition to locate faces\n",
    "    boxes = face_recognition.face_locations(rgb,model='hog')\n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    # loop over the encodings\n",
    "    for encoding in encodings:\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)\n",
    "#save emcodings along with their names in dictionary data\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "#use pickle to save data into a file for later use\n",
    "f = open(\"face_enc\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-component",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:52:59.986Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def recog():\n",
    "    \n",
    "    #find path of xml file containing haarcascade file \n",
    "    cascPathface = os.path.dirname(\n",
    "     cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "    # load the harcaascade in the cascade classifier\n",
    "    faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "    # load the known faces and embeddings saved in last file\n",
    "    data = pickle.loads(open('face_enc', \"rb\").read())\n",
    "    a=0\n",
    "    print(\"Streaming started\")\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    # loop over frames from the video file stream\n",
    "    while True:\n",
    "        # grab the frame from the threaded video stream\n",
    "        ret, frame = video_capture.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                             scaleFactor=1.1,\n",
    "                                             minNeighbors=5,\n",
    "                                             minSize=(60, 60),\n",
    "                                             flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        # convert the input frame from BGR to RGB \n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # the facial embeddings for face in input\n",
    "        encodings = face_recognition.face_encodings(rgb)\n",
    "        names = []\n",
    "        # loop over the facial embeddings incase\n",
    "        # we have multiple embeddings for multiple fcaes\n",
    "        for encoding in encodings:\n",
    "           #Compare encodings with encodings in data[\"encodings\"]\n",
    "           #Matches contain array with boolean values and True for the embeddings it matches closely\n",
    "           #and False for rest\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "             encoding)\n",
    "            #set name =inknown if no encoding matches\n",
    "            name = \"Unknown\"\n",
    "            # check to see if we have found a match\n",
    "            if True in matches:\n",
    "                #Find positions at which we get True and store them\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "                # loop over the matched indexes and maintain a count for\n",
    "                # each recognized face face\n",
    "                for i in matchedIdxs:\n",
    "                    #Check the names at respective indexes we stored in matchedIdxs\n",
    "                    name = data[\"names\"][i]\n",
    "                    #increase count for the name we got\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "                #set name which has highest count\n",
    "                name = max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "            # update the list of names\n",
    "            names.append(name)\n",
    "            # loop over the recognized faces\n",
    "            for ((x, y, w, h), name) in zip(faces, names):\n",
    "                # rescale the face coordinates\n",
    "                # draw the predicted face name on the image\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                 0.75, (0, 255, 0), 2)\n",
    "            a+=1\n",
    "        if a==5:\n",
    "            video_capture.release()\n",
    "            cv2.destroyAllWindows()            \n",
    "            return name\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-breathing",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:52:59.994Z"
    }
   },
   "outputs": [],
   "source": [
    "genuine_image_paths = \"Dataset/real/\"\n",
    "forged_image_paths = \"Dataset/forged/\"\n",
    "\n",
    "\n",
    "# ## Preprocessing the image\n",
    "\n",
    "\n",
    "def rgbgrey(img):\n",
    "    # Converts rgb to grayscale\n",
    "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[row])):\n",
    "            greyimg[row][col] = np.average(img[row][col])\n",
    "    return greyimg\n",
    "\n",
    "\n",
    "def greybin(img):\n",
    "    # Converts grayscale to binary\n",
    "    blur_radius = 0.8\n",
    "    # to remove small components or noise\n",
    "    img = ndimage.gaussian_filter(img, blur_radius)\n",
    "#     img = ndimage.binary_erosion(img).astype(img.dtype)\n",
    "    thres = threshold_otsu(img)\n",
    "    binimg = img > thres\n",
    "    binimg = np.logical_not(binimg)\n",
    "    return binimg\n",
    "\n",
    "\n",
    "def preproc(path, img=None, display=True):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    if display:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    grey = rgbgrey(img)  # rgb to grey\n",
    "    if display:\n",
    "        plt.imshow(grey, cmap=matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    binimg = greybin(grey)  # grey to binary\n",
    "    if display:\n",
    "        plt.imshow(binimg, cmap=matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    r, c = np.where(binimg == 1)\n",
    "    # Now we will make a bounding box with the boundary as the position of pixels on extreme.\n",
    "    # Thus we will get a cropped image with only the signature part.\n",
    "    signimg = binimg[r.min(): r.max(), c.min(): c.max()]\n",
    "    if display:\n",
    "        plt.imshow(signimg, cmap=matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    return signimg\n",
    "\n",
    "\n",
    "# ## Feature Extraction\n",
    "#\n",
    "\n",
    "\n",
    "def Ratio(img):\n",
    "    a = 0\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col] == True:\n",
    "                a = a+1\n",
    "    total = img.shape[0] * img.shape[1]\n",
    "    return a/total\n",
    "\n",
    "\n",
    "def Centroid(img):\n",
    "    numOfWhites = 0\n",
    "    a = np.array([0, 0])\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col] == True:\n",
    "                b = np.array([row, col])\n",
    "                a = np.add(a, b)\n",
    "                numOfWhites += 1\n",
    "    rowcols = np.array([img.shape[0], img.shape[1]])\n",
    "    centroid = a/numOfWhites\n",
    "    centroid = centroid/rowcols\n",
    "    return centroid[0], centroid[1]\n",
    "\n",
    "\n",
    "def EccentricitySolidity(img):\n",
    "    r = regionprops(img.astype(\"int8\"))\n",
    "    return r[0].eccentricity, r[0].solidity\n",
    "\n",
    "\n",
    "def SkewKurtosis(img):\n",
    "    h, w = img.shape\n",
    "    x = range(w)  # cols value\n",
    "    y = range(h)  # rows value\n",
    "    # calculate projections along the x and y axes\n",
    "    xp = np.sum(img, axis=0)\n",
    "    yp = np.sum(img, axis=1)\n",
    "    # centroid\n",
    "    cx = np.sum(x*xp)/np.sum(xp)\n",
    "    cy = np.sum(y*yp)/np.sum(yp)\n",
    "    # standard deviation\n",
    "    x2 = (x-cx)**2\n",
    "    y2 = (y-cy)**2\n",
    "    sx = np.sqrt(np.sum(x2*xp)/np.sum(img))\n",
    "    sy = np.sqrt(np.sum(y2*yp)/np.sum(img))\n",
    "\n",
    "    # skewness\n",
    "    x3 = (x-cx)**3\n",
    "    y3 = (y-cy)**3\n",
    "    skewx = np.sum(xp*x3)/(np.sum(img) * sx**3)\n",
    "    skewy = np.sum(yp*y3)/(np.sum(img) * sy**3)\n",
    "\n",
    "    # Kurtosis\n",
    "    x4 = (x-cx)**4\n",
    "    y4 = (y-cy)**4\n",
    "    # 3 is subtracted to calculate relative to the normal distribution\n",
    "    kurtx = np.sum(xp*x4)/(np.sum(img) * sx**4) - 3\n",
    "    kurty = np.sum(yp*y4)/(np.sum(img) * sy**4) - 3\n",
    "\n",
    "    return (skewx, skewy), (kurtx, kurty)\n",
    "\n",
    "\n",
    "def getFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    img = preproc(path, display=display)\n",
    "    ratio = Ratio(img)\n",
    "    centroid = Centroid(img)\n",
    "    eccentricity, solidity = EccentricitySolidity(img)\n",
    "    skewness, kurtosis = SkewKurtosis(img)\n",
    "    retVal = (ratio, centroid, eccentricity, solidity, skewness, kurtosis)\n",
    "    return retVal\n",
    "\n",
    "\n",
    "def getCSVFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    temp = getFeatures(path, display=display)\n",
    "    features = (temp[0], temp[1][0], temp[1][1], temp[2],\n",
    "                temp[3], temp[4][0], temp[4][1], temp[5][0], temp[5][1])\n",
    "    return features\n",
    "\n",
    "\n",
    "# ## Saving the features\n",
    "\n",
    "\n",
    "def makeCSV():\n",
    "    if not(os.path.exists('Dataset/Features')):\n",
    "        os.mkdir('Dataset/Features')\n",
    "        print('New folder \"Features\" created')\n",
    "    if not(os.path.exists('Dataset/Features/Training')):\n",
    "        os.mkdir('Dataset/Features/Training')\n",
    "        print('New folder \"Features/Training\" created')\n",
    "    if not(os.path.exists('Dataset/Features/Testing')):\n",
    "        os.mkdir('Dataset/Features/Testing')\n",
    "        print('New folder \"Features/Testing\" created')\n",
    "    # genuine signatures path\n",
    "    gpath = genuine_image_paths\n",
    "    # forged signatures path\n",
    "    fpath = forged_image_paths\n",
    "    for person in range(1, 13):\n",
    "        per = ('00'+str(person))[-3:]\n",
    "        print('Saving features for person id-', per)\n",
    "\n",
    "        with open('Dataset/Features/Training/training_'+per+'.csv', 'w') as handle:\n",
    "            handle.write(\n",
    "                'ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            # Training set\n",
    "            for i in range(0, 3):\n",
    "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',1\\n')\n",
    "            for i in range(0, 3):\n",
    "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',0\\n')\n",
    "\n",
    "        with open('Dataset/Features/Testing/testing_'+per+'.csv', 'w') as handle:\n",
    "            handle.write(\n",
    "                'ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            # Testing set\n",
    "            for i in range(3, 5):\n",
    "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',1\\n')\n",
    "            for i in range(3, 5):\n",
    "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',0\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TF Model\n",
    "\n",
    "\n",
    "def testing(path):\n",
    "    feature = getCSVFeatures(path)\n",
    "    if not(os.path.exists('Dataset/TestFeatures')):\n",
    "        os.mkdir('Dataset/TestFeatures')\n",
    "    with open('Dataset/TestFeatures/testcsv.csv', 'w') as handle:\n",
    "        handle.write(\n",
    "            'ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
    "        handle.write(','.join(map(str, feature))+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readCSV(train_path, test_path, type2=False):\n",
    "    # Reading train data\n",
    "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
    "    train_input = np.array(df.values)\n",
    "    # Converting input to float_32\n",
    "    train_input = train_input.astype(np.float32, copy=False)\n",
    "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
    "    temp = [elem[0] for elem in df.values]\n",
    "    correct = np.array(temp)\n",
    "    corr_train = keras.utils.to_categorical(\n",
    "        correct, 2)      # Converting to one hot\n",
    "    # Reading test data\n",
    "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
    "    test_input = np.array(df.values)\n",
    "    test_input = test_input.astype(np.float32, copy=False)\n",
    "    if not(type2):\n",
    "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
    "        temp = [elem[0] for elem in df.values]\n",
    "        correct = np.array(temp)\n",
    "        corr_test = kearas.utils.to_categorical(\n",
    "            correct, 2)      # Converting to one hot\n",
    "    if not(type2):\n",
    "        return train_input, corr_train, test_input, corr_test\n",
    "    else:\n",
    "        return train_input, corr_train, test_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(train_path, test_path, type2=False):\n",
    "    if not(type2):\n",
    "        train_input, corr_train, test_input, corr_test = readCSV(\n",
    "            train_path, test_path)\n",
    "    else:\n",
    "        train_input, corr_train, test_input = readCSV(\n",
    "            train_path, test_path, type2)\n",
    "    ans = 'Random'\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={\n",
    "                               X: train_input, Y: corr_train})\n",
    "            if cost < 0.0001:\n",
    "                break\n",
    "#             # Display logs per epoch step\n",
    "#             if epoch % 999 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(cost))\n",
    "#         print(\"Optimization Finished!\")\n",
    "\n",
    "        # Finding accuracies\n",
    "        accuracy1 = accuracy.eval({X: train_input, Y: corr_train})\n",
    "#         print(\"Accuracy for train:\", accuracy1)\n",
    "#         print(\"Accuracy for test:\", accuracy2)\n",
    "        if type2 is False:\n",
    "            accuracy2 = accuracy.eval({X: test_input, Y: corr_test})\n",
    "            return accuracy1, accuracy2\n",
    "        else:\n",
    "            prediction = pred.eval({X: test_input})\n",
    "            print(prediction)\n",
    "            if prediction[0][1] > prediction[0][0]:\n",
    "                print('Genuine Image')\n",
    "                ver=True\n",
    "                return True\n",
    "            else:\n",
    "                print('Forged Image')\n",
    "                ver=False\n",
    "                return False\n",
    "\n",
    "\n",
    "def trainAndTest(rate=0.001, epochs=1700, neurons=7, display=False):\n",
    "    start = time()\n",
    "\n",
    "    # Parameters\n",
    "    global training_rate, training_epochs, n_hidden_1\n",
    "    learning_rate = rate\n",
    "    training_epochs = epochs\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = neurons  # 1st layer number of neurons\n",
    "    n_hidden_2 = 7  # 2nd layer number of neurons\n",
    "    n_hidden_3 = 30  # 3rd layer\n",
    "\n",
    "    train_avg, test_avg = 0, 0\n",
    "    n = 10\n",
    "    for i in range(1, n+1):\n",
    "        if display:\n",
    "            print(\"Running for Person id\", i)\n",
    "        temp = ('0'+str(i))[-2:]\n",
    "        train_score, test_score = evaluate(train_path.replace(\n",
    "            '01', temp), test_path.replace('01', temp))\n",
    "        train_avg += train_score\n",
    "        test_avg += test_score\n",
    "    if display:\n",
    "        #         print(\"Number of neurons in Hidden layer-\", n_hidden_1)\n",
    "        print(\"Training average-\", train_avg/n)\n",
    "        print(\"Testing average-\", test_avg/n)\n",
    "        print(\"Time taken-\", time()-start)\n",
    "    return train_avg/n, test_avg/n, (time()-start)/n\n",
    "makeCSV()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-example",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:52:59.999Z"
    }
   },
   "outputs": [],
   "source": [
    "def signrecog(uid,filename):\n",
    "    global n_input\n",
    "    n_input = 9\n",
    "    global train_person_id\n",
    "    train_person_id = uid\n",
    "    global test_image_path\n",
    "    test_image_path = filename\n",
    "    global train_path\n",
    "    train_path = 'Dataset/Features/Training/training_' + \\\n",
    "        train_person_id+'.csv'\n",
    "    testing(test_image_path)\n",
    "    global test_path\n",
    "    test_path = 'Dataset/TestFeatures/testcsv.csv'\n",
    "    tf.reset_default_graph()\n",
    "    # Parameters\n",
    "    global learning_rate\n",
    "    learning_rate = 0.001\n",
    "    global training_epochs\n",
    "    training_epochs = 1000\n",
    "    global display_step\n",
    "    display_step = 1\n",
    "\n",
    "    # Network Parameters\n",
    "    global n_hidden_1\n",
    "    n_hidden_1 = 7  # 1st layer number of neurons\n",
    "    global n_hidden_2\n",
    "    n_hidden_2 = 10  # 2nd layer number of neurons\n",
    "    global n_hidden_3\n",
    "    n_hidden_3 = 30  # 3rd layer\n",
    "    global n_classes\n",
    "    n_classes = 2  # no. of classes (genuine or forged)\n",
    "\n",
    "    # tf Graph input\n",
    "    global X\n",
    "    X = tf.placeholder(\"float\", [None, n_input])\n",
    "    global Y\n",
    "    Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    global weights\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
    "    }\n",
    "    global biases\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
    "    }\n",
    "    # Construct model\n",
    "    global logits\n",
    "    logits = multilayer_perceptron(X)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    global loss_op\n",
    "    loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
    "    global optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    global train_op\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    # For accuracies\n",
    "    global pred\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    global correct_prediction\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    global accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Initializing the variables\n",
    "    global init\n",
    "    init = tf.global_variables_initializer()\n",
    "    return evaluate(train_path, test_path, type2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-pontiac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:53:00.005Z"
    }
   },
   "outputs": [],
   "source": [
    "from tkinter import filedialog\n",
    "chk=False\n",
    "def UploadAction(event=None):\n",
    "    filename = filedialog.askopenfilename()\n",
    "#     print('Selected:', filename)\n",
    "    if len(filename)!=0:\n",
    "        global checklabel\n",
    "        checklabel=Label(root, text='Processing...')\n",
    "        checklabel.grid(row=2,column=1)\n",
    "        if signrecog(e1.get(),filename):\n",
    "            checklabel.config(text='Success!')\n",
    "        else:\n",
    "            checklabel.config(text='Invalid!')\n",
    "def showoutput(event=None):\n",
    "    print(e1.get())\n",
    "def signrecogui(): \n",
    "    global root\n",
    "    root = Tk()\n",
    "    Label(root, text='ID').grid(row=0) \n",
    "\n",
    "    button = Button(root, text='Open', command=UploadAction).grid(row=1)\n",
    "\n",
    "    entry_var = StringVar()\n",
    "    global e1\n",
    "    e1 = Entry(root, textvariable=entry_var)\n",
    "    e1.grid(row=0, column=1) \n",
    "    e1.bind('<Return>',showoutput)\n",
    "    # if chk:\n",
    "    #     print(filename)\n",
    "    #     print()\n",
    "    #     signrecog(e1.get(),filename)\n",
    "         \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-plumbing",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-01T06:53:00.012Z"
    }
   },
   "outputs": [],
   "source": [
    "import time as tme\n",
    "master = Tk() \n",
    "Label(master, text='Username').grid(row=0) \n",
    "def settxt():\n",
    "    \n",
    "    if (uname.get()==recog()):\n",
    "        Label(master, text='Success').grid(row=2) \n",
    "        tme.sleep(3)\n",
    "        master.destroy()\n",
    "        signrecogui()\n",
    "Button(master, text='Login', width=25, command=settxt).grid(row=1)\n",
    "uname = Entry(master) \n",
    "\n",
    "uname.grid(row=0, column=1) \n",
    "mainloop() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
